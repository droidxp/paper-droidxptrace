\section{Discussion}\label{sec:discussion}

In this section, we answer our research questions,
summarize the implications of our results, and discuss possible
threats to the validity of the results presented so far.

\subsection{Answers to the Research Questions}

The results we presented in the previous sections
allow us to answer our four research questions, as
we summarize in the following.

\begin{itemize}
\item \textbf{Performance of the \mas on the \cds (RQ1).} 
  Our study indicates that the accuracy of the \mas reported in previous studies~\cite{DBLP:conf/wcre/BaoLL18,DBLP:journals/jss/CostaMMSSBNR22} does not
  generalize to a larger and more diverse dataset. That is, while in our
  reproduction study (using the \sds of previous research) the vanilla \mas
  leads to an accuracy of 0.89, we observed a drop of precision and recall
  that leads to an accuracy of 0.42 in the presence of our \cds (\apps pairs of
  original and repackaged versions of Android apps). 

\item \textbf{Effect of trace analysis (RQ2).} We do not find any gain of enriching the vanilla \mas with
  trace analysis in terms of accuracy ($F_1$ score). Although the use of traces
  reduces the number of false negatives (improving recall), it also increases the number of false
  positives with a similar proportion---which does not change the $F_1$ score significantly.
  Nonetheless, for the context of malware identification, we believe that the
  gain in terms of recall might justify the use of trace analysis.

\item \textbf{Similarity Analysis (RQ3).} Our results bring evidence about the existence of a negative
  association between the similarity of the original and repackaged versions of an app
  and the ability of the \mas to correctly
  classify a repackaged version of an app as a malware. Nonetheless,
  our similarity analysis alone was not sufficient to explain the low
  performance of the \mas to identify malware in the \cds.

\item \textbf{Malware Family Analysis (RQ4).} The results indicate that a specific family
  (\gps)  is responsible for the largest number of false
  negatives in the complete dataset. The \gps family corresponds to a particular type of
  Adware, designed to automatically display advertisements while an app is executing. After reverse engineering
  a sample of 20 \gps malware, we confirmed that the vanilla \mas and its trace variant cannot identify the
  patterns of changes introduced in the repackaged versions of the apps. The \gps family
  is the Achilles' heel of the \mas, since its accuracy increases substantially
  when we remove the \gps samples from our \cds.  
\end{itemize}


\subsection{Implications}\label{sec:implications} 

Contrasting to previous research works~\cite{DBLP:conf/wcre/BaoLL18,DBLP:conf/iceccs/LeB0GL18,DBLP:journals/jss/CostaMMSSBNR22},
our results 
%discussed in the previous sections 
lead to a more systematic understanding
of the strengths and limitations of using the \mas
for malware detection. In particular, this is the
first study that empirically evaluates the \mas
considering as ground truth the outcomes
of \vt. This decision allowed us to explore the
\mas performance using well-known accuracy metrics (i.e.,m precision, recall, and
$F_1$ score). Previous studies assume that all repackaged versions of the
apps were malware. Our triangulation with \vt reveals this is not true. Although
the \mas presents a good accuracy for the \sds ($F_1 = 0.89$), 
in the presence of a large dataset the \mas accuracy drops significantly ($F_1 = 0.42$). 

We also highlight that considering only the differences in the
sets of calls to sensitive APIs also leads to false negatives. We
argue in favor of using a more elaborate \emph{diff} approach, which
extends the mining sandbox approach to include the comparisons of
the dynamic call traces from the ``entry points'' of an Android app to its
set of calls to the sensitive APIs. Using this extension, we could reduce
the number of false negatives by a factor
between 14.33\% (\cds) to 71.42\% (\sds). We also reveal that
a specific family in the \cds is responsible for a large number of
false negatives. 
Altogether, the takeaways of this research are twofold:

\begin{itemize}
  \item Negative result: the mining sandbox approach for malware detection exhibits a much higher false negative rate than previous research reported. 

  \item Future directions: researchers should advance the mining sandbox approach for malware detection by exploring more advanced
    techniques for differentiating benign and malicious versions of the apps. In particular, new approaches should benefit from techniques
    that are able to identify particular patterns of changes in the repackaged versions. 
\end{itemize}  


\subsection{Threats to Validity}\label{sec:threats}

% \todo[inline]{MM: Needs major revision.}

There are some threats to the validity of our results.
Regarding {\bf external validity}, one concern relates to the 
representativeness of our malware datasets and how generic our findings are.
Indeed, mitigating this threat was one of the motivations for our research,
since, in the existing literature, researchers had explored just
one dataset of 102 pairs of benign/malign apps. Curiously,
for this small dataset, the performance of the
mining sandbox approach is more than twice superior
than its performance on our complete dataset (\apps pairs of
apps).

We contacted the authors of the Bao et al. research paper~\cite{DBLP:conf/wcre/BaoLL18}, asking them
if they had used any additional criteria for selecting their
dataset. Their answers suggest the contrary: they have not used
any particular app selection process that
could explain the superior performance of the mining
sandbox approach for the small dataset. We believe that
our results in the complete dataset generalize better than previous research works,
since we have a more comprehensive collection of malwares with different
families and degrees of similarity. Nonetheless, our
research focus only on Android repackaged malwares and thus we
cannot generalize our findings to malwares targeting
other platforms or that use different approaches to
generate a malicious asset.

Regarding {\bf conclusion validity}, during the exploratory phase of the mining sandbox approach,
we collected the set of calls to sensitive APIs the original version of
an app executes, while running a test case generation tool (such as
DroidBot). That is, the mining sandbox approach assumes the existence of a benign original
version of a given app in the exploratory phase. \highlight{We also query \vt to confirm this
assumption, and found that the original version of seven (out 102) apps in the
\sds contains malicious code. We believe the authors of previous studies carefully check that assumption,
and this difference had occurred because the outputs of \vt change over time~\cite{vt-label}}.
Therefore, while reproducing this research, it is necessary to query \vt to get the most
up-to-date classification of the assets, which might lead to results that might slightly
diverge from what we have reported here.

\begin{comment}
    
Regarding the \textbf{correlation between dataset properties and accuracy drop}, after running statistical tests (logistic regression),
we could not find evidence that the \emph{diversity} of the
complete dataset---in terms of similarity score and types of malware-
is responsible for the higher number of false negatives of the mining
sandbox approach. This implies that there was no 1-1 correlation between the brackets of similarity index, malware types to the drops in accuracy. Therefore, further research is necessary to investigate
other possible reasons for that. Perhaps, the complete dataset
contains a large percentage of malware that use more
advanced techniques to evade from both static and dynamic analysis---
both methods are used in the mining sandbox approach
we discussed in this paper.


\end{comment}


Regarding {\bf construct validity}, we address the main threats
to our study by using simple and well-defined metrics that are
in use for this type of research: number of correctly classified
samples in a dataset (true positives) and the number of assets that the
classification approaches we explored here fail to
correctly classify (false negatives). Based on these metrics we computed
the accuracy results using precision and recall. In a preliminary study, we
investigated whether or not the mining sandbox approach
would classify an original version of an app as a malware,
computing the results of the test case generation tools in multiple
runs. After combining three executions in an original version to
build a sandbox, we did not find any other execution that
could wrongly label an original app as a malware. 

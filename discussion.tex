\section{Discussion}

In this section, we explicitly answer our research questions,
summarize the implications of our results, and discuss possible
threats to the validity of the results presented so far.

\subsection{Answers to the Research Questions}

The results we presented in the previous sections
 answer our four research questions, as
we summarize in the following.

\begin{itemize}
\item \textbf{Performance of the \mas on the \cds (RQ1).} 
  Our study indicates that the accuracy of the \mas reported in previous studies~\cite{DBLP:conf/wcre/BaoLL18,DBLP:journals/jss/CostaMMSSBNR22} does not
  generalize to a larger and more diverse dataset. That is, while in our
  reproduction study (using the \sds of previous research) the vanilla \mas
  leads to an accuracy of 0.89, we observed a drop of precision and recall
  that leads to an accuracy of 0.42 in the presence of our \cds (\apps pairs of
  original and repackaged versions of Android apps). 

\item \textbf{Effect of trace analysis (RQ2).} We do not find any gain of enriching the vanilla \mas with
  trace analysis in terms of accuracy (F-score). Although the use of traces
  reduces the number of false negatives (improving recall), it also increases the number of false
  positives with a similar proportion---which does not change the $F_1$ measure significantly.
  Nonetheless, for the context of malware identification, we believe that the
  gain in recall might justify the use of trace analysis.

\item \textbf{Similarity Analysis (RQ3).} Our results bring evidence about the existence of a negative
  association between similarity and the ability of the \mas to correctly
  classify a repackaged version of an app as a malware. Nonetheless,
  our similarity analysis alone was not sufficient to explain the low
  performance of the \mas to identify malware in the \cds.

\item \textbf{Malware Family Analysis (RQ4).} The results indicate that a specific family
  (\gps)  is responsible for the largest number of false
  negatives in the complete dataset. The \gps family corresponds to a particular type of
  Adware, designed to automatically display advertisements while an app is executing. After reverse engineering
  a sample of 20 \gps malware, we confirmed that the vanilla \mas and its trace variant cannot identify the
  patterns of changes introduced in the repackaged versions of the apps. The \gps family
  is the Achilles' hell of the \mas, since its accuracy increases substantially
  when we remove the \gps samples from our \cds.  
\end{itemize}


\subsection{Implications}\label{sec:implications} 

Contrasting to previous research works~\cite{DBLP:conf/wcre/BaoLL18,DBLP:conf/iceccs/LeB0GL18,DBLP:journals/jss/CostaMMSSBNR22},
our results 
%discussed in the previous sections 
lead to a more systematic understanding
of the limitations of using the Android mining sandbox approach
for malware detection. In particular, we show that,
in the presence of a large dataset, the performance of the
approach drops significantly. 

We also highlight that considering only the differences in the
sets of calls to sensitive APIs also leads to false negatives. We
argue in favor of using a more elaborate \emph{diff} approach, which
extends the mining sandbox approach to include the comparisons of
the dynamic call traces from the ``entry points'' of an Android app to its
set of calls to the sensitive APIs. We collect these call traces while
building the sandbox in the usual way~\cite{DBLP:conf/icse/JamrozikSZ16}. Using this extension, we could improve
the performance of the vanilla mining sandbox approach by a factor
between 19\% (small dataset) to 22\% (complete dataset). We
also explored a more ``complementary'' approach, somewhat independent
of the mining sandbox approach, which statically search for two
specific patterns of changes in the Android manifest files: permission
duplication and component duplication. Using the manifest analysis,
we complement the performance of the mining sandbox approach, improving its performance by
a factor between 15\% (complete dataset) and 17\% (small dataset).
Altogether, the implications of this research are twofold:

\begin{description}
  \item[A warning to the community:] the mining sandbox approach for malware detection exhibits a much higher false negative rate  than previous research reported. 
  \item[Future directions:] researchers should advance the mining sandbox approach for malware detection by exploring more advanced techniques for differentiating benign and malicious versions of the apps. 
\end{description}  


\subsection{Threats to Validity}\label{sec:threats}

% \todo[inline]{MM: Needs major revision.}

There are some threats to the validity of our results.
Regarding {\bf external validity}, one concern relates to the 
representativeness of our malware datasets and how generic our findings are.
Indeed, mitigating this threat was one of the motivations for our research,
since, in the existing literature, researchers had explored just
one dataset of 102 pairs of benign/malign apps. Curiously,
for this small dataset, the performance of the
mining sandbox approach is more than twice superior
than its performance on our complete dataset (800 pairs of
apps). We contacted the authors of the Bao et al. research paper, asking them
if they have used any additional criteria for selecting the
dataset. Their answers suggest the contrary: they have not used
any particular app selection process that
could explain the superior performance of the mining
sandbox approach for the small dataset. We believe that
our results in the complete dataset (800 apps) generalize better than previous research works,
since we have a more comprehensive collection of malwares with different
categories and degrees of similarity. Nonetheless, our
research focus only on Android repackaged malwares and thus we
cannot generalize our findings to malwares targeting
other platforms or that use different approaches to
generate a malicious asset.

During the exploratory phase of the mining sandbox approach,
we collected the set of calls to sensitive APIs the benign version of
an app executes, while running a test case generation tool (such as
DroidBot). That is, the mining sandbox approach assumes the existence of a true benign
version of a given app in the exploratory phase. Regarding {\bf conclusion validity}, a
possible threat to our research is the use of \emph{pseudo-benign} versions
of some apps, which might compromise the results of our research.
To mitigate this threat, we query the VirusTotal repository to search for the
status of the benign and malicious versions of 100 apps,
randomly selected in our complete dataset. Among the benign
versions of these 100 apps, 14 apps were classified as a malware 
by at least two antivirus tools. When considering
the malicious version of these 100 apps, 44 apps were
classified as a malware by at least two antivirus tools. This might indicate
a possible misclassification in the AndroZoo repository---or a need
for an update. 

\begin{comment}
    
Regarding the \textbf{correlation between dataset properties and accuracy drop}, after running statistical tests (logistic regression),
we could not find evidence that the \emph{diversity} of the
complete dataset---in terms of similarity score and types of malware-
is responsible for the higher number of false negatives of the mining
sandbox approach. This implies that there was no 1-1 correlation between the brackets of similarity index, malware types to the drops in accuracy. Therefore, further research is necessary to investigate
other possible reasons for that. Perhaps, the complete dataset
contains a large percentage of malware that use more
advanced techniques to evade from both static and dynamic analysis---
both methods are used in the mining sandbox approach
we discussed in this paper.


\end{comment}



Regarding {\bf construct validity}, we address the main threats
to our study by using simple and well-defined metrics that are
in use for this type of research: number of correctly classified
malwares in a dataset (true positives) and the number of assets that the
classification approaches we explored here fail to
correctly classify (false negatives). Based on these metrics we computed
the recall of the approaches. We did not consider the
metric precision here, because the ``malicious'' assets in our
dataset are all true positives---hence precision would
always be of 100\%. In a preliminary study, we
investigated whether or not the mining sandbox approach
would classify a benign version of an app as a malware,
after executing the test case generation tools in multiple
runs. After combining three executions in a benign version to
build a sandbox, we did not find any other execution that
could wrongly label a benign app as a malware. So we are
confident that the mining sandbox approach leads to a
high precision, even though it has shown a poor recall
in our complete dataset. 


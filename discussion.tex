\section{Discussion}\label{sec:discussion}

In this section, we answer our research questions,
summarize the implications of our results, and discuss possible
limitations of our study that might threaten the
validity of the results presented so far.

\subsection{Answers to the Research Questions}

The results we presented in the previous sections
allow us to answer our three research questions, as
we summarize in the following.

\begin{itemize}
\item \textbf{Performance of the \mas on the \cds (RQ1).} 
  Our study indicates that the accuracy of the \mas reported in
  previous studies~\cite{DBLP:conf/wcre/BaoLL18,DBLP:journals/jss/CostaMMSSBNR22} does not
  generalize to a larger dataset. That is, while in our
  reproduction study (using the \sds of previous research) the \mas
  leads to an accuracy of \fscoreSmall, we observed a drop of precision and recall
  that leads to an accuracy of \fscore in the presence of our \cds (\apps pairs of
  original and repackaged versions of Android apps). 

%\item \textbf{Effect of trace analysis (RQ2).} We do not find any gain of enriching the vanilla \mas with trace analysis in terms of accuracy ($F_1$ score). Although the use of traces reduces the number of false negatives (improving recall), it also increases the number of false positives with a similar proportion---which does not change the $F_1$ score significantly. Nonetheless, for the context of malware identification, we believe that the gain in terms of recall might justify the use of trace analysis.

\item \textbf{Similarity Analysis (RQ2).} Our results bring evidence about the inexistence of an 
  association between the similarity of the original and repackaged versions of an app
  and the ability of the \mas to correctly
  classify a repackaged version of an app as a malware. Therefore,
  the similarity assessment does not explain the low
  performance of the \mas to identify malware in the \cds.

\item \textbf{Malware Family Analysis (RQ3).} The results indicate that a specific family
  (\gps)  is responsible for the largest number of false
  negatives in the complete dataset. The \gps family corresponds to a particular type of
  Adware, designed to automatically display advertisements while an app is running. After reverse engineering
  a sample of 30 \gps malware, we confirmed that the \mas cannot identify the
  patterns of changes introduced in the repackaged versions of the apps. The prevalence of the \gps family
  in the \cds desconstructs the performance of the \mas that previous research reported using
  small datasets of repackaged apps, mainly because the \mas recall increases substantially
  when we remove the \gps samples from our \cds.  
\end{itemize}


\subsection{Implications}\label{sec:implications} 

Contrasting to previous research works~\cite{DBLP:conf/wcre/BaoLL18,DBLP:conf/iceccs/LeB0GL18,DBLP:journals/jss/CostaMMSSBNR22},
our results 
%discussed in the previous sections 
lead to a more systematic understanding
of the strengths and limitations of using the \mas
for malware detection. In particular, this is the
first study that empirically evaluates the \mas
considering as ground truth the outcomes
of \vt. This decision allowed us to explore the
\mas performance using well-known accuracy metrics (i.e., precision, recall, and
$F_1$ score). Previous studies assume that all repackaged versions of the
apps were malware. Our triangulation with \vt reveals this is not true. Although
the \mas presents a good accuracy for the \sds ($F_1$ = \fscoreSmall), 
in the presence of a large dataset the \mas accuracy drops significantly ($F_1$ = \fscore). 
We also reveal that a specific family in the \cds is responsible for a large number of false negatives,
compromising the accuracy of the \mas.
Altogether, the takeaways of this research are twofold:

\begin{itemize}
  \item Negative result: the \mas for malware detection exhibits a much higher false negative rate than previous research reported. 

  \item Future directions: researchers should advance the \mas for malware detection by exploring more advanced
    techniques for differentiating benign and malicious versions of the apps. In particular, new approaches should benefit from techniques
    that are able to identify particular patterns of changes in the repackaged versions. 
\end{itemize}  


\subsection{Threats to Validity}\label{sec:threats}

% \todo[inline]{MM: Needs major revision.}

There are some threats to the validity of our results.
Regarding {\bf external validity}, one concern relates to the 
representativeness of our malware datasets and how generic our findings are.
Indeed, mitigating this threat was one of the motivations for our research,
since, in the existing literature, researchers had explored just
one dataset of 102 pairs of original/repackaged apps. Curiously,
for this small dataset, the performance of the
\mas is more than twice superior
than its performance on our \cds (\apps pairs of
apps).

We contacted the authors of the Bao et al. research paper~\cite{DBLP:conf/wcre/BaoLL18}, asking them
if they had used any additional criteria for selecting the pairs of apps in their
dataset. Their answers suggest the contrary: they have not used
any particular app selection process that
could explain the superior performance of the \mas for the small dataset. We believe that
our results in the complete dataset generalize better than previous research works,
since we have a more comprehensive collection of malware with different
families and degrees of similarity. Nonetheless, our
research focus only on Android repackaged malware and thus we cannot generalize
our findings to malware that targets other platforms or that use different approaches
to generate a malicious asset.

Regarding {\bf conclusion validity}, during the exploratory phase of the \mas, we collected the set of calls to sensitive APIs the original version of
an app executes, while running a test case generation tool (such as
DroidBot). That is, the \mas assumes the existence of a benign original
version of a given app in the exploratory phase. \highlight{We also query \vt to confirm this
assumption, and found that the original version of seven (out 102) apps in the
\sds contains malicious code. We believe the authors of previous studies carefully check that assumption,
and this difference had occurred because the outputs of \vt change over time~\cite{vt-label}}, and a dataset that is consistent at a date will not so in the future. Therefore, while reproducing this research, it is necessary to query \vt to get the most up-to-date classification of the assets, which might lead to results that might slightly
diverge from what we have reported here. Besides that, in the \cds we only consider
pairs of original/repackaged apps for which \vt classifies the original version as benign. 

\begin{comment}
    
Regarding the \textbf{correlation between dataset properties and accuracy drop}, after running statistical tests (logistic regression),
we could not find evidence that the \emph{diversity} of the
complete dataset---in terms of similarity score and types of malware-
is responsible for the higher number of false negatives of the mining
sandbox approach. This implies that there was no 1-1 correlation between the brackets of similarity index, malware types to the drops in accuracy. Therefore, further research is necessary to investigate
other possible reasons for that. Perhaps, the complete dataset
contains a large percentage of malware that use more
advanced techniques to evade from both static and dynamic analysis---
both methods are used in the mining sandbox approach
we discussed in this paper.


\end{comment}


Regarding {\bf construct validity}, we address the main threats to our study by using simple and
well-defined metrics that are in use for this type of research: number of malware samples the
\mas correctly/wrongly classify in a dataset (true positives/false negatives).
Based on these metrics, we computed the accuracy results using precision and recall. In a preliminary study, we
investigated whether or not the \mas would classify an original version of an app as a malware,
computing the results of the test case generation tools in multiple runs. After combining three executions
in an original version to build a sandbox, we did not find any other execution that could wrongly
label an original app as a malware. 

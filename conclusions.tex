\section{Conclusions and Future Work}\label{sec:conclusions}

%\todo[inline]{MM: Needs major revision.}

Mining sandbox is a popular approach to detect malware, especially the most prevalent type of malware, i.e., repackaged apps. There are several sandbox approaches, but past research has indicated that DroidBot is the most efficient of them all in terms of accuracy. In this paper, seek to understand the scalability of this accuracy. To this end, we first reproduce the results of DroidBot with the same dataset (vanilla dataset) as that was used in previous studies to validate our approach's authenticity. Then, we curate a larger dataset of app pairs (benign-malicious) that is 8X larger than the ones used in state-of-the-art studies. The dataset (complete dataset) is also incidentally diverse in its similarity index and malware kinds than the vanilla dataset. Our experiments with the complete dataset revealed that the accuracy of DroidBot drops significantly (~$24\%$ from ~$65\%$) thereby confirming our hypothesis that the accuracy of current mining sandbox approaches does not scale.
      
To mitigate this accuracy drop, we hypothesize two important extensions. First, we extend the mining sandbox approaches by making them aware of not only the difference in set of sensitive APIs present in each version, but also the differences between the dynamic call trace between the two app versions. Second, we further extend the approach by introducing a naive, yet impactful analysis of the differences in permissions requested between the two app versions. With both the extensions, we were able to achieve an accuracy increase of $19\%$ and $22\%$ on the vanilla and complete dataset, respectively. Although the final accuracy rate of the extended approach on the complete dataset is only $55.75\%$, our paper has provided valuable insights on the reliability of existing sandbox approaches and opened the gates for discussion on further strategies to mitigate this accuracy drop. 



\begin{comment}
In this paper, we conducted a study in which we observed the accuracy result, in terms of malware detection, of the state of the art mining sandbox approach. In our experiment, we used the test generator tool Droidbot to explore sensitive APIs called by malware at a real-word dataset of $800$, containing more representative samples than previous works. Our results demonstrated that the accuracy of Droidbot for malware detection drops to $24.12\%$ when we compare it with previous works, which explored a smaller sample. This first experiment also helps us to conclude that only a few sensitive APIs are responsible for most injected malware code at our repackaged sample.

These results encourage us to investigate how we could improve the mine sandbox approach. Hence, we investigated how relevant a dynamic call graph analysis and a simple manifest files analysis are to improving malware detection in the mine sandbox approach. Our investigation present relevant findings. First, with dynamic call graph analysis, we find that its possible to improve malware detection, if we consider trace analysis as a factor. Moreover, we present that a simple static analysis of manifest files also improves malware detection, complementing even the dynamic call graph analysis in terms of malware detection.

Our finds provide possible future research directions to further improve the mine sandbox approach. As future work, we plan to investigate if the distance at a dynamic call graph, between the Android app entry point and a call for a sensitive API has influence on the accuracy rate of malware detection at mine Android sandbox. We also plan to investigate the hypothesis that the test tool coverage used at mine Android Sandbox, can further benefit the approach since it provide more traces at dynamic call graph to inspect. Finally, we plan understand which are the malware features present at our dataset, that led accuracy of sandboxes approach drop significantly, when compared with previous work.
\end{comment}
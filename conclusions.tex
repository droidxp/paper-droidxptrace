%\section{Conclusions and Future Work}\label{sec:conclusions}
\section{Conclusions}\label{sec:conclusions}

%\todo[inline]{MM: Needs major revision.}

Sandbox mining is a popular approach to detect malware, especially the most prevalent type of malware, i.e., repackaged apps. While there are several such approaches, past research has indicated that DroidBot is the most efficient of them all in terms of accuracy. However, past studies of the accuracy of sandbox mining approaches used a small dataset. 
Hence, the goal of the work presented in this paper was to understand whether the previously reported accuracy generalizes to larger and more diverse datasets. To this end, we first reproduce the results of DroidBot with the same dataset (vanilla dataset) as that was used in previous studies to validate our approach's authenticity. Then, we curate a larger dataset of app pairs (benign-malicious) that is 8X larger than the ones used in state-of-the-art studies. The dataset (complete dataset) is also incidentally diverse in its similarity index and malware kinds than the vanilla dataset. Our experiments with the complete dataset revealed that the accuracy of DroidBot drops significantly (~$24\%$ from ~$65\%$) thereby confirming our hypothesis that the accuracy of current mining sandbox approaches does not scale.
      
To mitigate this accuracy gap, we hypothesized and validated two possible extensions. The first extension makes mining sandbox approaches aware of not only the difference in set of sensitive APIs present in each version, but also the differences between the dynamic call trace of the two app versions. The second extension introduces a simple analysis of the differences in permissions requested between the two app versions. With both these extensions in place, we were able to increase the accuracy of DroidBot by $19\%$ and $22\%$ on the vanilla and complete dataset, respectively. 
%Although the final accuracy rate of the extended approach on the complete dataset is only $55.75\%$, 
Overall, our paper has provided valuable insights on the reliability of existing sandbox approaches, hoping to trigger discussions on further strategies to mitigate this accuracy drop. 



\begin{comment}
In this paper, we conducted a study in which we observed the accuracy result, in terms of malware detection, of the state of the art mining sandbox approach. In our experiment, we used the test generator tool Droidbot to explore sensitive APIs called by malware at a real-word dataset of $800$, containing more representative samples than previous works. Our results demonstrated that the accuracy of Droidbot for malware detection drops to $24.12\%$ when we compare it with previous works, which explored a smaller sample. This first experiment also helps us to conclude that only a few sensitive APIs are responsible for most injected malware code at our repackaged sample.

These results encourage us to investigate how we could improve the mine sandbox approach. Hence, we investigated how relevant a dynamic call graph analysis and a simple manifest files analysis are to improving malware detection in the mine sandbox approach. Our investigation present relevant findings. First, with dynamic call graph analysis, we find that its possible to improve malware detection, if we consider trace analysis as a factor. Moreover, we present that a simple static analysis of manifest files also improves malware detection, complementing even the dynamic call graph analysis in terms of malware detection.

Our finds provide possible future research directions to further improve the mine sandbox approach. As future work, we plan to investigate if the distance at a dynamic call graph, between the Android app entry point and a call for a sensitive API has influence on the accuracy rate of malware detection at mine Android sandbox. We also plan to investigate the hypothesis that the test tool coverage used at mine Android Sandbox, can further benefit the approach since it provide more traces at dynamic call graph to inspect. Finally, we plan understand which are the malware features present at our dataset, that led accuracy of sandboxes approach drop significantly, when compared with previous work.
\end{comment}
\section{Results and discussion}\label{sec:results}

In this section, we detail the findings of our study.  We remind the reader that our main goal with this study is the detection of false negative in current mining sandbox approaches at a real-world and in-depth data set, and potential blindspots existing when detecting malware. Regarding blindspots, we hypothesize the presence of two major blindspots - trace taken by the app from the entry point to a sensitive API, and the differences in manifest file of repackaged apps. In Section~\ref{sec:testGeneration}, we summarize the results of our study that estimates the performance of Droidbot test generation tools for mine Android Sandboxes. For this purpose, we present the amount of app pair that had divergent sensitive app sets. We also present the sensitive APIs generally more used at repackaged apps. We performance this initial study since it help solve our R1, and served as reference to solve R2 and R3.

The remainder of this Section is structured as follows. Section~\ref{sec:Sensitive APIs} presents the false negative rate of our experiment, and top 15 most used sensitive APIs injected by malicious apps present at our dataset, answering R1. Section~\ref{sec:traceResults} presents the results of our study analysing the impact of trace on sandbox approaches to detect malware thereby answering R2, and Section~\ref{sec:manifestResults} presents the results of our study analysing the impact of modified manifest files on sandbox approaches to detect malware thereby answering R3. Section~\ref{sec:implications} presents some insights gained from the overall study and their potential implications.

\subsection{False negative rate and Sensitive APIs more used}\label{sec:Sensitive APIs}

In this section, we describe the results of reproducing the state of the art Android sandbox approaches on the new dataset of $800$ app pairs. We perform this experiment to ensure our analysis of the blind spots are done in the same playing field. Firstly, given a pair of apps, we first execute each version of app using test generation tool Droibot at DroidXP infrastructure for three minutes. We also repeat all this process three times.

After all executions, DroidXP produces a dataset with the sensitive methods that both app versions call at each execution. We consider that a test generation tool could construct a sandbox able to detect a specific malware, if a particular sensitive method is called only by the malicious version of the app. This check is done using a Python script that compares the set of sensitive methods, called at both app version (benign/malicious).

Hence, with DroidXP output we generate several reports based on union of each sensitive methods set, explored at each execution. In the end, It include a set of observations like set of sensitive methods access by each app version (benign/malicious), the sensitive methods access just for the malicious version (diff), and a summary that present at each rows: Test tool name used, app identification, and the number of different sensitive APIs find. This last information is crucial to calculate the false negative rate. If the set of sensitive methods access just by malicious app is empty, means that app is not a malicious, and therefore featuring a false negative.

With Droidbot, our results present that it could detect a total of $xx$ sensitive APIs set different among $800$ app pairs (xx.xx\%) explored. The result of Droidbot diverge from previous works that benefited from the same tool, as Bao et al. (66.66\%)~\cite{DBLP:conf/wcre/BaoLL18}, and Costa et al. (76.04\%)~\cite{DBLP:journals/jss/CostaMMSSBNR22}. However, at our work we also used the same app pair from these previous works, amount our $800$ app pair dataset. When we check these specific sample from these works, we realized that we get percentages close to these previous works, (xx.xx\%). 

In the end, after explorer all sensitive APIs set used by all app pair, we calculated which are the sensitive APIs more used for our repackage apps. At our study we find that when we used Droidbot to explore each repackage app, it calls xx times sensitive APIs from our list of 162 sensitive APIs, calling at least xx sensitive APIs one time. We realized that the first 15 APIs more used accounts for more than half of all calls (xx\%). We summarize our results at Table~\ref{tab:APIused}. 

With these results, we believe that mine sandbox approach could have a relevant false negative rate, resulting in a potential amount of \textit{blindspot}, which encouraged us to endorse efforts aimed at answering R2 and R3.

\begin{table*}[t]
  \caption{Sessitive APIs more used by repackage apps}
  \centering
  \begin{small}
 \begin{tabular}{lcc}
   \toprule
   Sensitive API & Occurrences & (\%) \\
   \midrule
   01 <android.telephony.TelephonyManager: java.lang.String getDeviceId()> &  60 & 5.40 \\
   02 <android.net.wifi.WifiManager: android.net.wifi.WifiInfo getConnectionInfo()> &  50 & 4.50 \\
   03 <android.net.wifi.WifiInfo: java.lang.String getMacAddress()> &  49 & 4.41 \\
   04 <android.net.NetworkInfo: java.lang.String getExtraInfo()> &  49 & 4.41 \\
   05 <android.telephony.TelephonyManager: java.lang.String getSubscriberId()> &  47 & 4.23 \\
   06 <android.net.NetworkInfo: java.lang.String getTypeName()> &  43 & 3.87 \\
   07 <android.net.NetworkInfo: android.net.NetworkInfo State getState()> &  38 & 3.42 \\
   08 <android.database.sqlite.SQLiteOpenHelper: android.database.sqlite.SQLiteDatabase getWritableDatabase()> &  35 & 3.15 \\
   09 <android.database.sqlite.SQLiteDatabase: android.database.Cursor query(java.lang.String, ...,java.lang.String)> &  35 & 3.15 \\
   10 <android.telephony.TelephonyManager: android.telephony.CellLocation getCellLocation()> &  34 & 3.06 \\
   11 <android.database.sqlite.SQLiteOpenHelper: android.database.sqlite.SQLiteDatabase getReadableDatabase()> &  34 & 3.06 \\
   12 <android.telephony.gsm.GsmCellLocation: int getLac()> &  33 & 2.97 \\
   13 <android.telephony.gsm.GsmCellLocation: int getCid()> &  33 & 2.97 \\
   14 <android.telephony.TelephonyManager: java.lang.String getNetworkOperator()> &  27 & 2.43 \\
   15 <android.telephony.TelephonyManager: java.lang.String getLine1Number()> &  26 & 2,34 \\
   
 \bottomrule

 \end{tabular}
 \end{small}
 \label{tab:APIused}
\end{table*}

\begin{obs}{1}{}
   %\kn{Here we need to add some final take aways of the reproduction study}
   Our results present that we do not have a high diversity of sensitive APIs explored by malicious app. We find that among xxx call to sensitive APIs, more than half were carried out to 15 sensitive APIs (Table~\ref{tab:APIused}). Results also suggest that mine sandbox approach could have a high false negative rate when applied on repackaged versions of benign apps. This encourages the emergence of new proposals that can support mine sandbox revealing eventual \textit{blindspot}.
 \end{obs}


%\kn{In this subsection, are we simply reproducing the results of existing papers. Because as far as I understand, tools like DroidBOT etc. were evaluated by simply comparing the sensitive APIs call. I am guessing here our contribution is to evaluate it on the larger dataset. I have given it a shot, please keep me posted if this is correct.}

\subsection{Trace Analysis Results}\label{sec:traceResults}

In this section, we describe the results of our investigation on how the trace from the entry point to sensitive API could impact the accuracy of sandbox approaches in terms of Android malware detection. Initially we collect the call graphs of Droidbot using \emph{Logcat}. We make a filter to collect just the traces between the app's entry point and calls to any sensitive method present at our previously list of $162$ sensitive methods.

Finally, with callgraph from both app versions (benign/malicious), we spot differences between their traces. We choose to investigate only app pairs that were the same sensitive APIs detected at both version at first experiment, i.e, from the point of view of sandbox approaches, it sheds light on the blind-spots. Our results show that among xx pair apps which had no difference between their sensitive APIs explored, xx of them (xx.xx\%) had different trace from the entry point to sensitive methods. This result which gives us evidence that it is possible to improve malware detection if we also consider the trace between app's entry point and calls to sensitive APIs.

Table~\ref{tab:pa} summarizes the results of this investigation. The column \textbf{Same API set (SAPI)} shows the number of app pair which had the same sensitive APIs explored at the first experiment, i.e, considered an undetected malware. The column  \textbf{Trace Different (TD)} shows the number of app pairs (among the undetected ones) that have different traces from the entry point to the sensitive method call. The column \textbf{Improv. \%} shows how much the malware detection of each tool could have been potentially improved if we considered the trace (calculated using ~\eqref{improve})

\begin{table}[ht!]
  \caption{Summary of the results of trace analysis. }
  \centering
  \begin{small}
 \begin{tabular}{lccc}
   \toprule
   Tool & Same API set (SAPI) & Trace Different (TD) & Improv. (\%) \\   \midrule
   Droidbot &  31 & 4 & 12.90 \\
 \bottomrule
 \end{tabular}
 \end{small}
 \label{tab:pa}
\end{table}



\begin{eqnarray}
Improvement & = & \frac{trace Different (TD) \times 100}{Execution (NID)} 
\label{improve}
\end{eqnarray}


Figure~\ref{fig:maliciousTrace} shows a example of trace injected at malicious version of the app \textbf{[com.android.remotecontrolppt]}. On that case, benign and malicious app version access the same sensitive method, \textit{getSubscriberid()}. This sensitive method returns the mobile unique subscriber ID, and require manifest file permission "READ\underline{\space}PHONE\underline{\space}STATE", present in both app version. The original app accesses this methods through $2$ traces (1 and 2), which suggest an aware action from app user. However, instead of the 2 original traces, the malicious version injected a third trace using as entry point a method which performs a stealth computation on a background thread, \textit{doInBackground}, suggesting a action without user aware.\fh{explain more about this example, and find another one }

\begin{figure}
\centering
\includegraphics[scale=0.30]{images/maliciousTrace_example01.pdf}
\caption{Example of Malicious Trace.}
 \label{fig:maliciousTrace}
\end{figure}

\begin{obs}{2}{}
 Test generation tools like Droidbot have a blind-spot when it comes to being aware of the trace taken from the entry point to a sensitive API call. Although Droidbot achieved better performance results when mining sensitives resources, it could have detected $xx\%$ of its undetectable app pairs had it considered trace as a factor.
 \end{obs}

\subsection{Manifest File Analysis Results}\label{sec:manifestResults}

In this section, we describe the results of our investigation on the impact of modified manifest files towards the accuracy of sandbox approaches. 
To this end, we check some particulars from Manifest file, that point to a likely suspicious behavior. In section \ref{sec:manifestAnalysis}, we illustrated that an automatic hacking script could inject duplicated permission and actions into the Manifest file. We looked out for such modifications in the malware that went undetected by the test generation tools. We also check if among these apps, there were requests to new permissions, that were not initially requested by the benign version, or if there were excessive requests for permissions in the malicious version's manifest files. Table~\ref{tab:mfa} summarizes our results. 

\begin{table}[ht]
  \caption{Manifest File with duplicate code.}
  \centering
  \begin{small}
 \begin{tabular}{lccccc}
   \toprule
   Tool & (NID) & (DP) & (DA) & (DP or DA) & (DP and DA) \\   \midrule
   Droidbot &  31 & 6 & 10 & 14 & 2 \\ 
   
 \bottomrule
 \end{tabular}
 \end{small}
 \label{tab:mfa}
\end{table}

The column (SAPI) indicates the number of malware that went undetected during our first study (same as Table~\ref{tab:pa}'s Same API set (SAPI)). The second column (DP) indicates how many Manifest files had duplicated permission. Column (DA) denotes the number of malware with the duplicated actions in their manifest file.

A duplicate request to permission in a malicious version's manifest file should have been performed by a script.  Droidbot could have detected $xx$ of their undetectable malware ($xx$) had it considered duplicate permissions or actions in their detection strategy.

Finally, we investigate how many app pairs request a suspicious amount of permissions. Past works indicate that a benign Android app normally requests on average $4$ permission, while a malicious apps version requests a median number of $7$ permission\cite{DBLP:conf/soups/FeltHEHCW12}\cite{DBLP:journals/tifs/0029LBKTLC17}. To this end, we observe how many app pairs have more than a delta of $3$ permission requests in comparison with the benign version. Table~\ref{tab:mp} presents the results.

\begin{table}[ht]
  \caption{Manifest File with suspicious amounts of permission requests.}
  \centering
  \begin{small}
 \begin{tabular}{lccc}
   \toprule
   Tool & (MP) & (DP) & (NDP) \\   \midrule
   
   Droidbot &  2 & 1 & 1 \\ 
   
 \bottomrule
 \end{tabular}
 \end{small}
 \label{tab:mp}
\end{table}

The second column (MP) present how many app pairs not detected by first experiment, have other suspicious number of permission requests ($3$ or more) in their manifest file. Third and fourth columns (DP)(NDP) presents how many app pairs out of these contain duplicated permission or not respectively. As we can see, just xx apps investigated had request a suspicious amount of permissions, xx with duplicated permission in your code, and other without.


\begin{obs}{3}{}
 We can conclude that sandbox approach also could had better accuracy if they considered the suspicious modifications at manifest file in their analysis.
\end{obs}

\subsection{Implications}\label{sec:implications}

The results presented so far have implications for both practitioners and researchers. We bring evidence that there are blindspots in Android sandbox approaches that if considered as a factor in malware identification, could improve mine sandbox technique. In a first step, we present that in our study, the accuracy of sandboxes approach was slightly lower than previous work. That is because our data set encompasses a wider range of malware, not considered at previous works. Our study points out that when use Droidbot test generator tool to explorer sensitive APIs at both versions, it was being able to identify $xx$ sensitive dataset distinct ($xx.xx$\%) in our dataset. We also show that among all sensitive APIs explored, xxxx was the API more injected at malicious app.

Next, we present a trace analysis at our data set which aims to investigate if there were some different traces between an entry point and call to any sensitive resource. We investigated traces, just from pair apps which we got different sensitive APIs set from first study. Our findings indicate that the trace analysis is also effective for malicious apps identification, and could support mine sandbox approach. Our First study had significant support of trace analysis when investigating the app pair with same sensitive APIs explored. Among $xxx$ app pair investigated, we present that $xx$ have different traces (\%xx), featuring therefore as a suspicious app. The message from these findings is that exploring trace analysis in conjunction with mine sandbox approach is useful for researchers and practitioners to improve malicious app detection tasks. 

Finally, the last study used static analysis at Android manifest file, to investigate if even naive techniques to construct malicious apps can go unnoticed for mine sandbox approach. In the previous section, Table~\ref{tab:mp} and~\ref{tab:mfa} summarizes this investigation. Although seems to be a simple technique, we reported some suspicious manifest files from malicious apps which also was a blindspots in Android sandbox approaches.

For industry and academia, these results have implications. They reinforce that there are benefits of integrating auxiliary techniques to mine sandbox approach for malware identification. We also present evidence that its possible benefit from static analysis on suspicious Android manifest file for malware identification, even at malware with a low similarity coefficient regarding its benign version. The Venn-diagram of Figure XXX summarizes how Droidbot explorer, the trace analysis, and two kinds of suspicious manifest files can complement each other.

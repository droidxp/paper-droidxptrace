\begin{abstract}

Android is by far the most popular operating system for the mobile platform and the ubiquitous nature of smartphones in our daily lives has only made its security a significant topic for researchers and practitioners alike. Previous research has shown that security experts can benefit from the mining sandbox approach based on sensitive APIs call to classify malware. This approach take advantage of test case generation tools to generate inputs to apps. In this paper, we present a negative result of previous work (SANER2018), which investigate accuracy of test case generation tools for mining sandbox. Even thought, the work find that Droidbot performs best in generating test cases, reporting a high accuracy rate of ~$70\%$, our investigation revealed that this accuracy is only valid on a smaller dataset of $102$ app pairs (benign-malicious). We have reached this conclusion when we try to understand which malware families can be uncovered by mining sandbox approach or not based on this previous work. For that, we reproduced the results in a larger (complete) dataset of $1204$ app pairs with a much more diverse similarity index and covering a broader range of malware kinds. To our surprise, our experiments revealed that the accuracy rate drops significantly to $28.32\%$.  We also open the discussion on the possible blindspots that plague mining sandbox approaches and their accuracy issues when scaled. To this end, we hypothesize the presence of one major blindspot with mining sandbox approaches; the divergent dynamic call traces between the two app versions. Our evaluation also show that when mining sandbox approaches are made aware of these blindspot, the accuracy rate improves, among other valuable insights for the research community.


\end{abstract}
\section{Experimental Setup}\label{sec:experimentalSetup}

The goal of this research is to build an in-depth understanding about
the performance of the \mas for detecting malware. To this
end, we conduct our research using a dataset of repackaged apps one order of magnitude
larger than previous studies~\cite{DBLP:conf/wcre/BaoLL18,DBLP:journals/jss/CostaMMSSBNR22}. Altogether, our
aim is to answer the following research questions:

\begin{enumerate}[(RQ1)]
\item \rqa
%\item \rqb
\item \rqc
\item \rqd  
\end{enumerate}

In this section, we describe our study settings. First, we present how we mined the samples of Android apps that we
use as a dataset for our study (Section~\ref{sec:dataset}).  Then, we describe the data
collection and data analysis procedures in Sections~\ref{sec:dataCollectionProc} and~\ref{sec:dataAnalysisProc}.


\subsection{Malware Dataset}\label{sec:dataset}

We use a curated Android app dataset collected by~\cite{DBLP:journals/tse/LiBK21}, based on the Androzoo repository~\cite{DBLP:conf/msr/AllixBKT16}. It contains 15,297 repackaged apps, by over 100 different attackers, for
2,776 original apps. It extends a previous dataset (hereafter \sds) used in the research works of Bao et al. and Costa et al.~\cite{DBLP:conf/wcre/BaoLL18,DBLP:conf/scam/CostaMCMVBC20}, which contains a set of 102 pairs of \emph{original} and \emph{repackaged} apps. We curate our dataset selecting a random sample from the initial 15,297 repackaged apps, built a final dataset (hereafter \cds) of \apps apps that we were able to instrument and execute our experiments.


%starting from an initial sample of 3344 repackaged pairs of apps available in AndroZoo~\cite{DBLP:conf/msr/AllixBKT16}.
%We do not use any particular criteria for selecting the initial sample.
%Nonetheless, due to compatibility issues we found---either during the instrumentation phase (using DroidFax) or during the execution
%phase using the Android emulator---we end-up with our final dataset (hereafter \cds) that contains \apps pairs of
%repackaged apps (36\% of the initial subset repackaged apps).
%\fh{At this paragraph I change the term set to Subset since 3344 is a subset of 15.000 repackage pair}

%\kn{This whole part about Virustotal needs a bit more elaboration. Ideally, including a citation explaining why we go for this additional method of classifying whether something as a malware or not. Since we already start with the app pairs, we sort of already know the malicious and benign version right? }
We queried the \vt repository to find out which repackaged apps in our
dataset have been indeed labeled as a malware. \vt is a well-known mechanism for
scanning software assets (such as Android apps) using more than 30 anti-virus engines~\cite{DBLP:journals/ese/KhanmohammadiEH19}.
%, and we took this decision since the output of \vt can change over time~\cite{vt-label}.\fh{Here I inserted a litle discription of
% VT and inserted some reference}
%(\kn{two out of how many antiviruses are included by Virustotal}). 
According to \vt, in the \sds (102 pairs),
69 of the repackaged apps (67.64\%) have been identified as a malware by at least two
\ses. Here we only consider that a repackaged version of an app is a malware if \vt reports that at least
two \ses identify a malicious behavior within the asset. This is in accordance with previous research~\cite{vt-label,DBLP:journals/ese/KhanmohammadiEH19}. Conversely, considering the \cds, at least two \se identified \malwares out of the \apps repackaged apps as malware (\malwaresP\%).

Classifying malware into different categories is a common practice. For instance, Android malware can be classified into categories
like riskware, trojan, adware, etc. Each category might be further specialized in several malware families, depending of its
characteristics and atack strategy---e.g, steal network info (IP, DNS, WiFi), collect phone info,
collect user contacts, send/receive SMS, and so on~\cite{DBLP:conf/iccns/RahaliLKTGM20}.
According to the
\avt~\cite{avclass2-paper}, the malware samples in the \sds come from $17$ different families---most of them from the Kuguo (49.27\%) and Dowgin (17.39\%) families.
Contrasting, our \cds is more representative. Besides a large sample of repackaged apps (\apps in total), it
comprises 34 families of malware we collected using the \avt---most
of them from the Gappusin (37.00\%), Revmob (20.69\%), and Dogwin (10.87) families.

We also characterize our dataset according to the similarity
between the original and repackaged versions of the apps, using the  
SimiDroid tool~\cite{DBLP:conf/trustcom/0029BK17}. SimiDroid quantifies the similarity
based on (a) the methods that are either identical or similar in both versions of the apps (original and repackaged versions),
(b) methods that only appear in the repackaged version of the apps (new methods), and (c) methods that only appear in the
original version of the apps (deleted methods).

Our \cds has an average similarity score of 63.59\%---with a much better distribution (252 of
app pairs have a similarity score of less than 0.25\%, 160 of app pairs
between 0.25\% and 0.50\%, 232 of the apps between 0.5\% and 0.75\%,
and 575 of the apps with more than 0.75\%). The \sds presents a much higher
similarity index average (77.87\%). 

It is important to highlight that our sample comes
from different Android app stores. Most of our repackage apps come from a non-official
Android app store, Anzhi~\cite{anzhi}. However some repackaged apps also come from the
official Android app store, Google Play.


%\begin{figure}[ht]
%\centering
%\includegraphics[scale=0.43]{images/stores.pdf}
%\caption{Markets where malware was discovered.}
% \label{fig:stores}
%\end{figure}


\subsection{Data Collection Procedures} \label{sec:dataCollectionProc}

We take advantage of the DroidXP infrastructure~\cite{DBLP:conf/scam/CostaMCMVBC20}
for data collection. DroidXP allows researchers to compare 
test case generation tools in terms of malicious app behaviors identification, using the \mas. Although the comparison of test
case generation tools is not the goal of this paper, DroidXP
was still useful for automating the following steps of our study.


\begin{enumerate}[S1]
 \item \textbf{Instrumentation}: In the first step,
we configure DroidXP to instrument all pairs of apps in our dataset.
Here, we instrument both versions of the apps (as APK files) to collect relevant information during their execution. Under the hood, DroidXP leverages
DroidFax~\cite{DBLP:conf/icsm/CaiR17a} to instrument the apps and collect static
information about them. To improve the performance across multiple executions,
this phase executes only once for each version of the apps in our dataset.

\item \textbf{Execution}: In this step, DroidXP first installs the (instrumented) version of the APK files in the Android emulator we use in our experiment (API 28) and then starts a test case generation tool for executing both apps version (benign and malicious). We execute the apps via Droidbot~\cite{DBLP:conf/icse/LiYGC17}, mostly because previous works report the best accuracy of the sandboxes built using the \mas and the DroidBot as test case generation tool. To also ensure that each execution gets the benefit of running on a fresh Android instance without biases that could stem out of history, DroidXP wipes out all data stored on the emulator that has been collected from previous executions.


\item \textbf{Data Collection}: During the execution of the instrumented apps, we collect all relevant information (such as calls to sensitive APIs, test coverage metrics, and so on). We use this information to analyse the performance of the \mas for detecting malicious behavior.
\end{enumerate}

\subsection{Data Analysis Procedures} \label{sec:dataAnalysisProc}

We use basic statistics (average, median, standard deviation) to identify the
accuracy of the \mas to identify malware, in both
datasets we use in our research---i.e., the \sds
with 102 pairs of apps and our \cds with
\apps pairs. We use the Spearman Correlation~\cite{spearman-correlation} method and
Logistic Regression~\cite{statistical-learning} to understand the strengths of
the associations between the similarity index of a
malware with the \mas accuracy---that is,
if the approach was able to correctly classify an asset as malware. We also use existing tools to reverse engineer a sample of repackaged
apps in order to better understand (the lack of) accuracy
of the \mas.

%\fh{I removed the item C of this section that talk about hardware and execution time.}

%\subsection{Environment Configuration}\label{sec:hardware}

%\todo[inline]{RB: later, if we need space, we can safely comment this section out.}

%We deployed our experiment on a 32-Core, AMD EPYC 7542 CPU, 512 GB RAM, storage Samsung SSD 970 EVO 1TB machine running a 64-bit Debian  GNU/Linux 11. We also configured our emulator to run all selected apps on Google Android version 9.0, API 28, 512M SD Card, 7GB internal storage, with X86 ABI image.
%For our study, we configured DroidXP to run each of the \apps app pairs using DroidBot for 3 minutes. To mitigate noise, we repeated the full process 3 times,  which took around 421 machine hours in total. Although it was possible to run more than 10 emulators in parallel on one physical machine, to avoid any interference resulting from context switching within the operating system, we chose to run one emulator at a time. \rb{I don't know if the following is relevant\ldots} Hence, all processes took around 22 days, 17 days for experiment execution and additional 5 days for environment configuration.


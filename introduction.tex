\section{Introduction}\label{sec:introduction}

Mobile technologies like smartphones and tablets have become fundamental to the way we function as a society. Almost two-thirds of the world population
uses mobile technologies~\cite{Comscore,DBLP:journals/tse/MartinSJZH17}, with the
Android Platform dominating this market accounting for more than 70\% of the \emph{mobile
market share} with almost 3.5 million Android applications~\footnote{In this paper, we will use the terms Android Applications, Android Apps and Apps interchangeably, to refer to Android software applications} (apps)
available on the Google Play Store~\cite{Statista}. 
With increased popularity also comes increased risk of attacks ---motivating plenty of research efforts to design and develop new techniques
to identify malicious behavior or vulnerable code in Android apps~\cite{DBLP:conf/msr/SamhiBK22} 
\fh{inserted this reference}

For instance, the
mining sandbox approach takes advantage of automated test case generation tools 
to explore the behavior of an app---in terms of calls to sensitive APIs---and then
generates an Android sandbox~\cite{DBLP:conf/icse/JamrozikSZ16}. During a normal
execution of the app, the sandbox might block any call to a sensitive API
that had not been observed during the exploratory phase. 
Researchers have shown that the mining sandbox approach is also effective
in detecting a popular class of Android malware that ``repackages'' benign apps~\cite{DBLP:conf/wcre/BaoLL18,le2018towards}---i.e., starting with a benign version of an app from an official app store, such as Google Play, one might infect it with
malicious code, such as broadcasting
sensitive information to a private server~\cite{DBLP:journals/tse/LiBK21}. The infected app is then
shared with users using different app stores. 
Previous research works~\cite{DBLP:conf/wcre/BaoLL18,DBLP:journals/jss/CostaMMSSBNR22} compare the accuracy of Android sandboxes for malware detection 
produced from the execution of different test case generation tools, including Monkey~\cite{Monkey}, Droidbot~\cite{DBLP:conf/icse/LiYGC17}, and Droidmate~\cite{DBLP:conf/kbse/BorgesHZ18} tools.
These reports bring evidence that the test generation tool Droidbot outperforms the other tools, leading to sandboxes with an accuracy
rate close to 70\%. 

Although revealing promising results, the previous research has two major concerns. First, they use a small dataset of malware comprising only 102 pairs of benign/malicious versions of an app. Second, their impact analysis is not fine-grained. For instance, they donot consider whether the impact on the accuracy that might be due to malware characteristics, such as the similarity index between the benign and the repackaged malicious version of an app and the malware family (gappusin, kuguo, dowgin, etc.).
\fh{change family name}In this paper we present the results of an investigation that aims to replicate previous studies~\cite{DBLP:conf/wcre/BaoLL18,DBLP:conf/scam/CostaMCMVBC20} in a larger dataset of malware, and then explore whether a more diverse sample of app pairs has an influence on the accuracy of the mining sandbox approach for malware identification. To this end, we explore the performance of the mining sandbox approach using the most accurate test case generation tool, Droidbot and a new dataset we curated for this research. Our new dataset is an order of magniture larger (it contains \apps pairs of benign/malicious apps), comprises of a much more diverse similarity index, and covers a broader range of malware families. 



{\bf Negative results.} To our surprise, the experimental results show that, on a larger and more diverse dataset (compared to previous studies), the mining sandbox approach's accuracy rate drops significantly (from $70\%$ to $28.76\%$). This result motivated us to conduct a series of experiments to understand the reasons for the lower accuracy in the larger dataset and also explore an extension of the original mining sandbox approach for malware identification. The original approach classifies an app as malware whenever there exists a difference between the sets of calls to sensitive APIs collected when running the test case generation tool over the original and repackaged versions of the same app. In our extension, we also classify as malware a version of an app that calls the same set of sensitive APIs collected in the exploratory phase but with at least one different trace from the app's entry points to the collected sensitive method calls.


Our experiments show that the accuracy of sandbox approaches does indeed improve, when they are made aware of the differences in the dynamic call trace, improving accuracy by $16.12\%$ (from $28.76\%$ to $44.88\%$). 
An accuracy of $44.88\%$, may still appear unsatisfactory for a truly trustworthy sandbox approach. Still, 
our experiments open the discussion (a) on one important blindspot that must be considered when building 
malware detection approaches that mine sandboxes, and (b) on the need to investigate further sources of information when trying to distinguish benign and malign app versions. 

In fact, during our analysis we observed another interesting insight about a specific malware family described as \textit{gappusin}. Out of the total of 198 \textit{gappusin} family samples, 171 samples ($86.36\%$), neither the original mining sandbox approach nor the dynamic call trace analysis were able to identify an app repackage version as a malware. This indicate that researchers may want to pay more attention to this malware family to improve the security of Android apps.
\fh{I did a little update at this paragraph, to talk about gappusin family}

% to improve the security of Android apps. \\

%% \noindent
%% In summary, we make the following contributions:

%% \begin{enumerate}[1.]
%% \item We presented a negative result of the state of the art in mining sandbox approaches, using DroidBot as a test cases generator at a larger-scale and more realistic dataset, compared with previous works that used a small and less representative dataset.
%% \item A systematic investigation of the effect of trace analysis on improving the accuracy of the sandbox approach.
%% \item An in-depth look into the kind of sensitive APIs that plague most repackaged apps.
%% \item A reproduction package of our studies is made available online; scripts for statistic analysis are also available.%\footnote{https://github.com/droidxp/paper-droidxp-replication.git}
%% \end{enumerate}

The rest of the paper is organized as follows. We begin with background and related work in Section \ref{sec:background};
readers familiar with mining Android sandboxes may skip this section.
Section \ref{sec:experimentalSetup} describes our infrastructure and methodology. Section \ref{sec:results} presents our results and qualitative analysis. Section \ref{sec:discussion} answer our research questions, present some implications and discusses thread to validity. Section \ref{sec:conclusions} 
presents some potential areas for future work and concludes the paper.
%\kn{Here, please describe each section in one sentence}

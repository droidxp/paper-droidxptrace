\section{Introduction}\label{sec:introduction}

Almost two-thirds of the world use mobile technologies, such as smartphones and tablets, becoming a central role in everyday life in the last decade~\cite{Comscore,DBLP:journals/tse/MartinSJZH17}. In this context, Android Operation System has dominated this market, with around 3.29 million Android application~\footnote{In this paper, we will use the terms Android Applications, Android Apps and Apps interchangeably, to refer to Android software applications} (apps) available on the Google Play Store on first quarter of 2022~\cite{Statista}. Due to this growing popularity, we witnessed an unprecedented growth of  Android apps that perform malicious activities (malware).  This in turn has made security issues in Android apps a relevant research topic. As such, several techniques have emerged to identify malicious behavior or vulnerable code in Android apps, including static analysis algorithms to reveal private information leaks~\cite{DBLP:conf/pldi/ArztRFBBKTOM14} or misuse of 
cryptographic primitives.~\cite{DBLP:journals/tse/KrugerSABM21}

Another alternative for protection from Android malicious behavior consists in the use of dynamic analysis to mine Android sandboxes~\cite{DBLP:conf/icse/JamrozikSZ16}. Such approaches use automated testing tools (i.e., dynamic analysis) to explore apps behavior in terms of access to sensitive resources. Researchers have shown that the mining sandbox approach is also effective to detect a popular class of Android malware based on repackaging~\cite{DBLP:conf/wcre/BaoLL18,le2018towards}---that is, starting with a benign version of an app from an official app store, such as Google Play, one might inject it with code performing malicious activities such as broadcasting credit card information to a private server.~\cite{DBLP:journals/tse/LiBK21} %(\kn{Here add a citation that shows prevalence of repackaged malware}).

Previous work show that DroidBot is the state of the art test case generation tool for malware identification using the mining sandbox approach~\cite{DBLP:conf/wcre/BaoLL18,DBLP:journals/jss/CostaMMSSBNR22}, achieving an accuracy rate between 65\% and 75\%. But their evaluation was performed on a set of $102$ and $96$ app pairs~\footnote{App pairs here refers to pairs of the same app containing a benign and a malicious repackaged version} respectively. Our internal analysis also revealed that the similarity index between the app pairs used in DroidBot's evaluation was $77\%$. Nonetheless, previous results from Bao et al.~\cite{DBLP:conf/iceccs/LeB0GL18} %(\rb{we need to confirm that this is the correct reference. Not sure if (\cite{DBLP:conf/wcre/BaoLL18}) presents a discussion about similarity.}) 
have revealed that malware tend to be harder to detect if the similarity index is lower. Both the above mentioned points indicate that the relatively high accuracy reported in earlier evaluations of state of the art mining sandbox approaches for malware detection may not scale to larger datasets with a more diverse and potentially higher similarity index.

In this paper, we address two open questions that arise. First, we seek to understand what the accuracy of the state of the art in malware detection using the mining sandbox approaches, DroidBot, is when presented with a larger dataset ($800$ as opposed to the $102$). This new dataset also has a combined average similarity index of $62.47\%$ which is lower than the original $77\%$. Secondly, if the accuracy rate has indeed significantly dropped, during the reproduction of DroidBot with a more real-world representative dataset, what approaches can improve upon the accuracy. 

To answer the first question, we present the results of a study where we observed the accuracy results of DroidBot on a set from $800$ real app pairs (benign/malicious). For this reproduction study, we take advantage of DroidXP~\cite{DBLP:conf/scam/CostaMCMVBC20}, a tool suite that supported us not only to integrate DroidBot but also in our study setup and data collection. To answer the second question, we hypothesize that the mining sandbox approach for malware detection using tools like DroidBot performs a superficial analysis of differences between app pairs, i.e., only the difference in the set of sensitive APIs called. We assert that there are two major blindspots in the state of the art:

\begin{enumerate}
    \item The differences between two apps that call the same set of sensitive APIs, but differ in their dynamic call traces between the app's entry point and the sensitive APIs.
    \item The {\color{blue}change patterns} within the Android Manifest' section that requests permissions, when considering the two variants of apps (i.e., benign and malicious variants).
\end{enumerate}

To explore the item (1), we used an auxiliary tool from DroidXP project called DroidXPTrace, which compares all traces from an entry point to sensitive APIs call and check for possible trace divergences. To {\color{blue}mine change patterns} in the Android Manifest files, we harness the standard APK analysis tool from Android~\cite{au2011short}. Altogether, we investigate the following specific questions in our study, for which we provide answers in Section~\ref{sec:results}.

\todo[inline]{\rb{I still have to review the RQs}}

\begin{enumerate}[(RQ1)]
 \item How well does the state of the art in mining sandbox approaches, DroidBot, perform with a large dataset of app pairs with a diverse similarity index?
 \item How relevant is a dynamic call trace analysis to improving malware detection in support of the mine sandbox approach?
 \item How relevant is a manifest files analysis to improving malware detection in support of the mine sandbox approach?
\end{enumerate}
 

Our findings indicate that the accuracy of DroidBot significantly drops (to $24.12\%$) when reproduced on a larger dataset of $800$ app pairs with a similarity index of $62.47\%$. Our experiments also reveal that the accuracy of sandbox approaches improve when they are made aware of the differences in the dynamic call trace and difference in permissions requested using the manifest file. Specifically, we improved the accuracy when we made aware of differences in the dynamic call trace at $22\%$ ($24.12\%$ to $46.12\%$), and more $9.63\%$ ($46.12\%$ to $55.75\%$) if we also consider difference in permissions request. %\kn{Please add here the results of path analysis and manifest files individually and their intersection}
During our analysis we also observed another interesting insight that out of the 162 sensitive APIs explored, just 16 ($10.49$\%) were injected at most repackaged apps 
%\rb{(are all subjects in AndroZoo based on repackaging?)}. 
It indicates that we have a small set of sensitive APIs used generally by malicious developers, pointing out that researchers can concentrate their efforts on this specific set of sensitive APIs to improve the security of Android apps. 
In summary, this paper makes the following contributions:

\begin{enumerate}[1.]
\item A reproduction of the state of the art in mining sandbox approaches, DroidBot scaled in terms of number of app pairs and similarity index.
\item A broad comprehension about the role of trace analysis and static analysis on Android manifest files in improving the accuracy Android sandbox approach.
\item An in depth look into the kind of sensitive APIs that plague most repackaged apps.
\item A reproduction package of the studies online. Scripts for statistic analysis are also available.\footnote{https://github.com/droidxp/paper-droidxp-replication.git}
\end{enumerate}


The rest of the paper is organized as follows. We begin with background and related work in Section \ref{sec:background}. Reader familiar with dynamic and static analysis on Android apps and mine Android sandbox may wish start at Section \ref{sec:experimentalSetup}, which describes our infrastructure and methodology. Section \ref{sec:results} presents our results and qualitative analysis. Section \ref{sec:threats} discusses thread to validity, and Section \ref{sec:conclusions} present some future work and concludes the paper.
%\kn{Here, please describe each section in one sentence}





\section{Introduction}\label{sec:introduction}

Existing reports discuss that almost two-thirds of the world population
uses mobile technologies~\cite{Comscore,DBLP:journals/tse/MartinSJZH17}, such as smartphones and tablets, with the
Android Platform dominating this market. Indeed, Android controls more than 70\% of the \emph{mobile
market share} and there are almost 3.5 million Android applications~\footnote{In this paper, we will use the terms Android Applications, Android Apps and Apps interchangeably, to refer to Android software applications} (apps)
available on the Google Play Store~\cite{Statista}. 
Nonetheless, perhaps due to its increasing popularity, the number of atacks and  
malicious software (malware) targeting the Android platform have also
increased---motivating a plenty of research efforts to design and develop new techniques
to identify malicious behavior or vulnerable code in Android apps.

For instance, the so called
mining sandbox approach takes advantage of automated test case generation tools 
to explore the behavior of an app---in terms of calls to Android sensitive APIs---and then
generates an Android sandbox~\cite{DBLP:conf/icse/JamrozikSZ16}. During a normal
execution of the app, the sandbox might block any call to a sensitive API
that had not been observed during the exploratory phase. 
Researchers have shown that the mining sandbox approach is also effective
in detecting a popular class of Android malware that ``repackages'' benign apps~\cite{DBLP:conf/wcre/BaoLL18,le2018towards}---i.e., starting with a benign version of an app from an official app store, such as Google Play, one might infect it with
malicious code, such as broadcasting
sensitive information to a private server~\cite{DBLP:journals/tse/LiBK21}. The infected app is then
shared with users using different app stores. 
Previous research works~\cite{DBLP:conf/wcre/BaoLL18,DBLP:journals/jss/CostaMMSSBNR22} compare the accuracy of Android sandboxes for malware detection 
produced from the execution of different test case generation tools, including Monkey~\cite{Monkey}, Droidbot~\cite{DBLP:conf/icse/LiYGC17}, and Droidmate~\cite{DBLP:conf/kbse/BorgesHZ18} tools.
These reports bring evidence that the test generation tool Droidbot outperforms the other tools, leading to sandboxes with an accuracy
rate close to 70\%. 

Although revealing promising results, the previous research fails concerning two perspectives. First, they use a small dataset of malware comprising only 102 pairs of benign/malicious versions of an app. Second, they do not investigate the impact on the accuracy that might be due to malware characteristics, such as the similarity index between the benign and the repackaged malicious version of an app and the malware family (Trojan, Adware, SmsSend, etc.). So, in this paper we present the results of an investigation that aims to replicate previous studies~\cite{DBLP:conf/wcre/BaoLL18,DBLP:conf/scam/CostaMCMVBC20} in a larger dataset of malware, and then explore if a more diverse sample of app pairs has an influence on the accuracy of the mining sandbox approach for malware identification. To this end, we explore the performance of the mining sandbox approach using the Droidbot test case generation tool and a new dataset we curate for this research. Our new dataset is one order of magniture larger (it contains \apps pairs of benign/malicious apps), presents a much more diverse similarity index, and covers a broader range of malware families. 



{\bf Negative results.} To our surprise, the experiment results show that, on a larger and more diverse dataset (compared to previous studies), the mining sandbox approach's accuracy rate drops significantly --- compared to what has been reported previously. This result motivated us to conduct a series of experiments to understand the reasons for the lower accuracy in the larger dataset and also explore an extension of the original mining sandbox approach for malware identification. The original approach classifies an app as malware whenever exists a difference between the sets of calls to sensitive APIs collected when running the test case generation tool over the original and repackaged versions of the same app. In our extension, we also classify as malware a version of an app that calls the same set of sensitive APIs collected in the exploratory phase, though presenting at least one different trace from the app's entry points to the collected sensitive method calls.


Our experiments show that the accuracy of sandbox approaches does indeed improve, when they are made aware of the differences in the dynamic call trace, improving accuracy by $16.12\%$ (from $28.76\%$ to $44.88\%$). 
An accuracy of $44.88\%$, may still appear unsatisfactory for a truly trustworthy sandbox approach. Still, 
our experiments open the discussion (a) on one important blindspot that must be considered when building 
malware detection approaches that mine sandboxes, and (b) on the need to investigate further sources of information when trying to distinguish benign and malign app versions. 
In fact, during our analysis we observed another interesting insight that out of the total of $162$ sensitive APIs, just less than half ($64$) were injected at most repackaged apps. 
This indicates that a small set of sensitive APIs is used by malicious developers and hence 
researchers may want to pay more attention to this specific set of sensitive APIs.\\

% to improve the security of Android apps. \\

%% \noindent
%% In summary, we make the following contributions:

%% \begin{enumerate}[1.]
%% \item We presented a negative result of the state of the art in mining sandbox approaches, using DroidBot as a test cases generator at a larger-scale and more realistic dataset, compared with previous works that used a small and less representative dataset.
%% \item A systematic investigation of the effect of trace analysis on improving the accuracy of the sandbox approach.
%% \item An in-depth look into the kind of sensitive APIs that plague most repackaged apps.
%% \item A reproduction package of our studies is made available online; scripts for statistic analysis are also available.%\footnote{https://github.com/droidxp/paper-droidxp-replication.git}
%% \end{enumerate}

The rest of the paper is organized as follows. We begin with background and related work in Section \ref{sec:background};
readers familiar with mining Android sandboxes may skip this section.
Section \ref{sec:experimentalSetup} describes our infrastructure and methodology. Section \ref{sec:results} presents our results and qualitative analysis. Section \ref{sec:threats} answer our research questions, present some implications and discusses thread to validity. Section \ref{sec:conclusions} 
presents some potential areas for future work and concludes the paper.
%\kn{Here, please describe each section in one sentence}

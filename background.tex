\section{Background and Related Work}\label{sec:background}

%% In this section, we introduce the concepts and terminology that are necessary to understand the reminder of this paper. First, Section~\ref{sec:sand} introduces some background information about \emph{sandboxes} within the security context. Section~\ref{sec:repackage} presents background information about repackaged application and how they introduce malicious behavior.
%% Finally, in Section~\ref{sec:android-sandbox} we review the \emph{mining sandbox approach} for detecting repackaged Android apps.

There are many tools that favor developers to reverse engineering the Android bytecode language~\cite{DBLP:conf/issta/WangGMC15}.
For this reason, software developers can easily decompile trustworthy apps, modify their contents by inserting malicious code,
repackage them with malicious payloads, and re-publish them in app stores, including official ones like the Google Play Store.
It is well-known that repackaged Android apps can leverage the popularity of real apps to increase its propagation and spread malware.  
Repackaging has been raised as a noteworthy security concern in Android ecosystem by stakeholders in the app development industry and researchers. Indeed, there are reports claiming that about 25\% of Google Play Store app content correspond to repackaged apps~\cite{DBLP:conf/sigmetrics/ViennotGN14}. Nevertheless, all the workload to detect and remove malware from markets by the stores (official and non-official ones), have not been accurate enough to address the problem. As a result, repackaged Android apps threaten security and privacy of unsuspicious Android app users, beyond compromising the copyright of the original developers~\cite{DBLP:journals/access/KimLCP19}. Aiming at
mitigating the threat of malicious code injection in repackaged apps,
several techniques based on both static and dynamic analysis of Android apps have been proposed,
including the \mas for malware classification~\cite{DBLP:conf/icse/JamrozikSZ16,DBLP:conf/wcre/BaoLL18}. 

\subsection{Mining Android Sandboxes}\label{sec:android-sandbox}

A \emph{sandbox}
is a well-known mechanism to secure a system and forbid a software component from accessing
resources without appropriate permissions. Sandboxes have also been used to build an isolated
environment within which applications cannot affect other programs, the network, or other device
data~\cite{DBLP:journals/peerj-cs/MaassSCS16}. The idea of using sandboxes emerged from the
need to test unsafe software, possible malware, without worrying about the integrity of the
device under test~\cite{DBLP:conf/esorics/BordoniCS17}, shielding the operating system from security issues.
To this end, a sandbox environment should have the minimum requirements to run the
program, and make sure it will never assign the program greater privileges than it should have,
respecting the \emph{least privilege} principle.
% This principle ensures unauthorized access to resources,
% improving the system's overall health.
Within the Android ecosystem, sandbox approaches ensure the principle
of the \emph{least privilege} by preventing apps from having direct access to resources like device hardware (GPS, Camera), or sensitive data from other apps. Access to sensitives data
like contacts list or resources are granted through specific APIs (Application Programming Interface), known as sensitive APIs, which are managed by by coarse-grained Android permissions system~\cite{DBLP:journals/corr/abs-2109-06613}.



%% The main market source for Android apps is Google Play Store. Unfortunately, it has
%% a flexible policy regarding the process of publishing apps, and therefore, many Android apps are removed from the
%% store because of issues related to malware\cite{DBLP:conf/msr/WangLL0X18}. Google Play tries
%% to minimize unauthorized access to sensitive resources by malicious apps,
%% listing each app with its requested permission. {\color{red}Those permissions are presented to Android
%% users at app installation moment since version 6}. However, some works presented that most users are careless regarding these permissions since they are only interested to run the app~\cite{DBLP:conf/soups/FeltHEHCW12}. This represents a great security breach since malware usually asks for more permissions than their APIs normally would require~\cite{DBLP:conf/ccs/FeltCHSW11}.

%% \begin{figure}[ht]
%% \centering
%% \includegraphics[scale=0.35]{images/mineSandbox.pdf}
%% \caption{Mine Sandbox.}
%%  \label{fig:mineSandbox}
%% \end{figure}

The \mas~\cite{DBLP:conf/icse/JamrozikSZ16} aims at automatically
building a sandbox through dynamic analysis (i.e., using automatic test generation tools).
The main idea is to grant permissions to an app based on its calls to sensitive APIs.
Thus, sandboxes build upon these calls to create safety rules and then block future
calls to other sensitive resources, which diverge from those found in the first exploratory
phase. Using the Droidmate test generation tool~\cite{DBLP:conf/icse/JamrozikZ16},
Jamrozik et al. proposed a full fledged
implementation of the \mas, named Boxmate~\cite{DBLP:conf/icse/JamrozikSZ16}. 
Boxmate records the occurrences of calls to sensitive APIs from native code, and the UI events that triggers these calls,
like button clicks. It is possible to configure Boxmate to record events associated with each sensitive call as
tuples (event, API), instead of recording just the set of calls to sensitive APIs. Jamrozik et al. argue that, in this way, Boxmate generates finer
grain results which
might improve the accuracy of the \mas, even with the presence of reflection---which is quite commonly used in
malicious apps~\cite{DBLP:conf/issta/0029BOK16}.

In fact, the \mas can be implemented using
a mix of static and dynamic analysis. In the first phase, one
can instrument an Android app to log any call to the Android sensitive methods.
After that, one can execute a test case generation tool (such as DroidBot
or Monkey) to explore the app behavior at runtime,
while the calls to sensitive APIs are recorded.
%Figure~\ref{fig:mineSandbox} presents this general approach for \mas.
This set of calls to sensitive APIs is then used
to configure the sandbox. The general \mas % (see Figure~\ref{fig:mineSandbox})
suggests that the more efficient the test generator tool (for instance, in terms of code coverage),
the more accurate would be the resulting sandbox.


%\todo[inline]{Since this figure is not discussed in the paper, we can remove it without any problem. I have enriched the previous paragraph, though, to make it
%  more necessary. Nonetheless, I will change this figure a bit, in order to
%represent the instrumentation phase.}

\subsection{Mining Android Sandbox for Malware Identification}

Besides being used to generate Android sandboxes, the \mas is also effective 
to detect if a repackaged version of an Android app contains malicious
behavior~\cite{DBLP:conf/wcre/BaoLL18}. In this scenario, the \emph{effectiveness} of the approach
is estimated in terms of the accuracy in which malicious behavior is correctly identified in the repackaged version of the
apps.


%Figure~\ref{fig:sensitiveAPI} illustrates the \mas for
%repackaged apps identification.
%We leverage DroidXP~\cite{DBLP:conf/scam/CostaMCMVBC20} to collect the set of sensitive APIs
%the versions of the apps call (benign/malign versions).
The \mas for malware classification typically works as follows. In a first step ({\bf instrumentation phase}), 
a tool instruments the code of the apps (both original and repackaged versions) to collect relevant information
during the apps execution in later stages. Then, in a second step ({\bf exploration phase}),
we collect a set $S_1$ with all calls to sensitive APIs the original version of an app executes while running a test case generator tool (like DroidBot).
In the third step ({\bf exploitation phase}), we (a) collect a set $S_2$ with all calls to sensitive APIs the repackaged version of an app
executes while running a test case generator tool and then (b) computes the set $S = S_2 \setminus S_1$ and check whether  $S$ is empty or not.
The \mas classifies the repackaged version as a malware whenever $|S| > 0$.  

%%We configure DroidXP to run DroidBot for $3$ minutes. Since some malware may use dynamic features (such as reflection) to introduce malicious behavior,
%%which can change apps behavior at runtime~\cite{DBLP:journals/spe/ZhangLTX18,DBLP:journals/tosem/LiTX19}, this second
%%analysis is also important to disclose some sensitive APIs calls ignored by first analysis (static analysis).

%%While our static analysis is made once, we execute dynamic analysis $3$ times. The result of static analysis and all executions is finally joined, forming a final set containing all calls to sensitive APIs coming from the original version of the app, as described at Figure~\ref{fig:sensitiveAPI}: ($S_{A}$, $D_{A1}$, $D_{A2}$, $D_{A3}$). We carry out the same procedure for the malicious version of the apps, creating a distinct set of calls to sensitive APIs (now coming from the malicious version of the apps). In a final step, we compare the two sets of calls to sensitive APIs ($Set_B, Set_M$). We use the following rules to check for a malicious behavior. 

%% \begin{enumerate}
%%     \item If the difference between the two final sets is an empty set, we cannot distinguish the benign from the malicious version of the app (false negative).
%%     \item Otherwise, we successfully distinguish the benign from the malicious version of the app (true positive). 
%%     %\kn{Cant we replace the first part of the second point simply as "Otherwise" or is there some specific corner cases that I am missing}
%% \end{enumerate}

%% In addition, this procedure also enables us to identify the calls to sensitive APIs that are more frequently injected by the malicious version of the apps
%% in our dataset. 

%% %\fh{here I have to insert the figure}

%%\begin{figure}[ht]
%%\centering
%%\includegraphics[scale=0.45]{images/sensitiveAPIdiff.pdf}
%%\caption{All procedure for suspicious app identification %%using sensitive API set diff.}
%%\label{fig:sensitiveAPI}
%%\end{figure}



%\kn{I have commented out the subsection here as it seems redundant with the previous subsection}
%\subsection{The Mining Android Sandbox Approach for Malware Identification}

%The focus of our paper is in approaches that mine android sandboxes to classify Android Malware.
%There is a vast body of research in this direction. 

Previous research works reported the results of empirical studies that aim to investigate the effectiveness of
the \mas for malware classification~\cite{DBLP:conf/wcre/BaoLL18,DBLP:conf/scam/CostaMCMVBC20}.
For instance, Bao et al. found that, in general, the sandboxes constructed using test generators classify at least 66\% of repackaged apps as malware in a
dataset comprising 102 pairs of apps (original/repackaged versions)~\cite{DBLP:conf/wcre/BaoLL18}.
Actually, the mentioned work performed two studies: one pilot study involving a dataset
of 10 pairs of apps (\texttt{SmallE}), in which the authors executed each test case generation tools for one hour; and a larger experiment
(\texttt{LargeE}) involving 102 pairs of
apps in which the authors executed the test case generation tools for one minute~\cite{DBLP:conf/wcre/BaoLL18}.

Here we replicate their larger experiment. 
The authors also presented that, among five test generation tools used, DroidBot~\cite{DBLP:conf/icse/LiYGC17} leads to the most effective sandbox.
Le et al. extend the \mas for malware classification with additional verification,
such as the values of the parameters used in the
calls to sensitive APIs~\cite{le2018towards}, while
Costa et al.\cite{DBLP:journals/jss/CostaMMSSBNR22} investigated the impact of static analysis to complement the accuracy of the \mas
for malware classification. Their study reports that DroidFax~\cite{DBLP:conf/icsm/CaiR17a}, the static analysis infrastructure used in~\cite{DBLP:conf/wcre/BaoLL18}, classifies as malware almost half of the repackaged apps.
%in a dataset of $96$ pairs of
%benign/malicious apps.

%\todo[inline]{I could not understand why we did not cite our JSS work here}
%However, none of the aforementioned studies
% ~\cite{DBLP:conf/icse/JamrozikSZ16,DBLP:conf/wcre/BaoLL18,le2018towards}
%neither characterize the APIs included on the repackage versions nor investigate the
%possibility that trace analysis using call graph or analysis of
% the manifest file could complement the mining sandbox approach for malware identification.


Our work, although closely related to previous studies, differs from them in several aspects.  First, our assessment is more comprehensive: instead of considering 102 pairs of original/repackaged apps, we execute our study considering \apps pairs of apps.
%Curiously, the performance of the \mas in our large dataset drops significantly.
We then investigate which characteristics of the malware samples in the large dataset contribute to the performance of the \mas for malware classification.


%\subsection{Extending the Mining Sandbox with Trace Analysis}


%In order to conduct the trace analysis, we first compute a dynamic call graph (CG) that characterizes the execution of each version of the apps in our dataset. Our goal here is to explore how many pairs of apps call the same set of sensitive APIs, though using different call graph traces. We hypothesize that differences in the traces might be used to complement the \mas to identify suspicious behavior. As such, here we execute the trace analysis for all app pairs of our dataset, in order to reduce the number of repackaged apps incorrectly labeled as benign (false negative). For detecting different traces, we performed a comparison of the dynamic call graph of each pair of app. Our procedure checks if there is some additional CG trace from the entry points to the calls to sensitive APIs that occurs in the repackaged version but not in the original version of an app.

%\rb{(Not sure if this is the right decision. I do not see any problem in running
%  this study for all pairs.)}. 
%% we  investigate those app pairs that were not described as a malware during the exploratory step, i.e,
%% the test generation tool DroidBot collected the same set of sensitive APIs for both version. If a dynamic call graph
%% of these app pairs presented different traces from entry point to sensitive APIs call at both versions, we suspect this to indicate presence of malware. Figure~\ref{fig:callGraph} illustrates an example of benign and repackaged call graphs. In this example, although both app versions access the same set of sensitive APIs, the malicious version presents an additional execution trace. 


%\begin{figure}[ht]
%\centering
%\includegraphics[scale=0.30]{images/maliciousCallGraph.pdf}
%\caption{Illustrative example of the trace analysis. In this case, both %versions call the same set of sensitive APIs. Nonetheless,
%the traces between the entry point and the calls to sensitive APIs %diverge.}
% \label{fig:callGraph}
%\end{figure}


%For instance, Figure~\ref{fig:maliciousTrace} shows an example of a trace injected in a repackaged version of the app \textbf{[com.android.remotecontrolppt]}. Here, the original and repackaged versions access the same sensitive method, \textit{getSubscriberid()}. This sensitive method returns the device's unique subscriber ID. The original app accesses this method through two distinct traces (Trace 01 and Trace 02), which suggests an expected use case. However, besides the two original traces, the repackaged version injects a third trace (Trace 03) containing as entry point a method that performs a stealth computation on a background thread, \textit{doInBackground}, suggesting an action without user's awareness.


%\begin{figure}
%\centering
%\includegraphics[scale=0.28]{images/maliciousTrace_example01.pdf}
%\caption{Example of Malicious Trace.}
% \label{fig:maliciousTrace}
%\end{figure}

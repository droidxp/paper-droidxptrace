\section{Related Work}\label{sec:relatedwork}
In this section, we discuss prior studies in two areas: Dynamic and Static Analysis on Android apps and mine Sandbox.

\subsection{Dynamic and Static Analysis on Android apps}\label{sec:analysis}

There is a large body of work that explores the use of program analysis techniques to detect malware. 

Several works have been proposed to detect malware based on sensitive method calls and permission control~\cite{DBLP:conf/mobicom/WeiGNF12,DBLP:conf/asiajcis/WuMWLW12,DBLP:conf/sp/LiDLDG21}. Cai et al.~\cite{DBLP:journals/tse/CaiR21} presented a longitudinal study on Android apps focusing on run-time behaviors. However, this work does not focus specifically on malware detection but on general security gaps in apps by considering only benign apps. Fangfang et al.~\cite{DBLP:conf/wisec/ZhangHZW014} proposed ViewDroid, which models the UIs of Android apps as a directed graph. Although ViewDroid also works by comparing app pairs to identify repackaged apps, their focus is UI centric.

On static analysis approaches exploring Android Manifest files, Kim et al. proposed RomaDroid~\cite{DBLP:journals/access/KimLCP19}.  Their approach does not consider the structural context in Manifest files, but rather treat the files as sequence of strings and perform a lowest common subsequence (LCS) based approach to detect repackaged apps. Au el al.~\cite{DBLP:conf/ccs/AuZHL12} also apply static analysis on Android Manifest files to detect vulnerabilities in Android apps. They do this by mapping requested permissions to sensitive API calls in the code.

Li et al.~\cite{DBLP:journals/tifs/0029LBKTLC17} provided a systematic knowledge on Android malware by conducting an empirical study comparing malicious repackage app with their benign counterparts (1,497 app pairs). They found that the majority of Android malware are repackaged versions of benign apps that donot do anything complex modifications, many times simply reusing library code.

 In the domain of detecting repackaged apps by comparing app pairs, Crussell et al.~\cite{DBLP:conf/esorics/CrussellGC12} proposed  DNADroid, which compares program dependence graphs, and Zhou et al.~\cite{DBLP:conf/codaspy/ZhouZJN12} DroidMoss which detects and analyzes repackaged apps using a fuzzy hashing technique.

\kn{We should add some lines here why we did not use any of these approaches and why our list Droidbot etc. is different from there. Maybe mention something in the lines of "How state of the art our tools are"?}
\subsection{Mine Sandbox}\label{sec:mineSandbox}

The focus of our paper is approaches that mine android sandboxes and generate tests to detect Android Malware. There is a vast body of research in this direction. Using a test generation tool named Droidmate~\cite{DBLP:conf/icse/JamrozikZ16}, Jamrozik et al.~\cite{DBLP:conf/icse/JamrozikSZ16} proposed the first mainstream Sandbox approach for detecting Android Malware, called Boxmate. 

Boxmate records the occurrences of calls to sensitive APIs and the event that trigger these calls, like button click. Boxmate records events associated with each sensitive call as tuples (event, API). Jamrozik et al. imply that using this finer granularity results in lower false alarm rates, even with the presence of reflection which is quite commonly used in malicious apps~\cite{DBLP:conf/issta/0029BOK16}. 

However, unlike our approach, Boxmate was tested only using 12 malware apps and do not consider trace  between entry point (event) and access to resource sensitive (call API) as a factor.

Bao et al.~\cite{DBLP:conf/wcre/BaoLL18} conducted an empirical study to investigate the effectiveness of android sandbox approaches, exploring test generation tools, including Droidmate. The authors found that in general, the sandboxes constructed by test generator tools can detect more than $70$\% malicious apps in a dataset comprising $102$ pairs (benign/malicious). The study also presented that among 5 test generation tools used, DroidBot~\cite{DBLP:conf/icse/LiYGC17} constructed the most efficient sandbox. Le et al.~\cite{le2018towards} extend the work of Bao et al. by combining more categories of APIs, and also considering the impact of parameters.

None of the above mentioned studies ~\cite{DBLP:conf/icse/JamrozikSZ16}~\cite{DBLP:conf/wcre/BaoLL18}~\cite{le2018towards} investigated the possibility that trace analysis using call graph or analysis of the manifest file could complement mining sandbox approaches. Hence, our work although related closely to aforementioned studies, differs from them in four aspects: First, these approaches used a small sample of app pairs thereby shedding doubts into the generality of the conclusions. To improve the study, we conducted our work exploring $95$ pair of apps and also included a set of sensitive methods containing a total of 162 methods, 65 more than Bao et al. study, which used just the sensitive APIs definiden in the AppGuard privacy-control framework\cite{DBLP:conf/esorics/BackesGHMS13}. Second, we explored the possibility of false positives at sandbox approach, which was not explored at previous studies. Third, we explore the impact of dynamic call graphs, exploring traces from entry point to sensitive API access. Last but not least, we also consider explore suspicious extraction of features from Manifest file apps using a simple static analysis approach.